import os
import ray
import time
import json
import threading
import logging
from enum import Enum
from datetime import datetime
import numpy as np

from ray.rllib.algorithms.ppo import PPOConfig
from ray.rllib.models.torch.torch_modelv2 import TorchModelV2
from ray.rllib.models import ModelCatalog
from ray.rllib.utils.framework import try_import_torch
from ray.rllib.policy.sample_batch import concat_samples

# ---- æ§åˆ¶ Ray æ—¥å¿— ----
os.environ["RAY_DEDUP_LOGS"] = "0"
logging.getLogger("ray").setLevel(logging.ERROR)

torch, nn = try_import_torch()
F = nn.functional


# ============================================================
# ä¸‰ç§æ¨¡å¼
# ============================================================
class CompileMode(Enum):
    NONE = "none"
    SYNC = "sync"
    ASYNC = "async"


# ============================================================
# çº¯ PyTorch çš„å‰å‘éª¨å¹²ï¼ˆç”¨äºè¢« torch.compileï¼‰
# æ¥å£ï¼šforward(obs: Tensor) -> (logits, value)
# ============================================================
class PolicyBackbone(nn.Module):
    def __init__(self, in_dim: int, num_outputs: int, hidden_dim: int = 64):
        super().__init__()
        self.fc1 = nn.Linear(in_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.policy_head = nn.Linear(hidden_dim, num_outputs)
        self.value_head = nn.Linear(hidden_dim, 1)

    def forward(self, obs: torch.Tensor):
        x = torch.relu(self.fc1(obs))
        x = torch.relu(self.fc2(x))
        logits = self.policy_head(x)
        value = self.value_head(x)  # [B, 1]
        return logits, value


# ============================================================
# è‡ªå®šä¹‰ PPO Policy æ¨¡å‹ï¼ˆRLlib æ¥å£ï¼‰
# - è®­ç»ƒæ°¸è¿œç”¨ self.backboneï¼ˆæœªç¼–è¯‘ç‰ˆæœ¬ï¼‰
# - æ¨ç†å¯ä»¥ç”¨ compiled_backboneï¼ˆç”±æˆ‘ä»¬æ§åˆ¶ï¼‰
# ============================================================
class CustomPolicyNet(TorchModelV2, nn.Module):
    def __init__(self, obs_space, action_space, num_outputs, model_config, name):
        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)
        nn.Module.__init__(self)

        in_dim = obs_space.shape[0]
        self.in_dim = in_dim
        self.num_outputs = num_outputs

        # æœªç¼–è¯‘çš„è®­ç»ƒç”¨ backbone
        self.backbone = PolicyBackbone(in_dim, num_outputs)
        # å¯é€‰çš„ç¼–è¯‘å backboneï¼Œä»…ç”¨äºæ¨ç†
        self.compiled_backbone = None
        self.use_compiled = False

        self._value_out = None

    def forward(self, input_dict, state, seq_lens):
        obs = input_dict["obs"]
        # ç»Ÿä¸€è½¬æˆ tensorï¼Œé¿å… numpy å‚ä¸è®¡ç®—å›¾
        if not isinstance(obs, torch.Tensor):
            obs = torch.as_tensor(obs, dtype=torch.float32)
        else:
            obs = obs.float()

        # é€‰æ‹©å½“å‰ä½¿ç”¨çš„å‰å‘æ¨¡å—ï¼ˆè®­ç»ƒ or æ¨ç†ï¼‰
        bb = self.compiled_backbone if (self.use_compiled and self.compiled_backbone is not None) else self.backbone

        logits, value = bb(obs)
        # logits: [B, num_outputs], value: [B,1]
        self._value_out = value.view(-1)  # [B]
        return logits, state

    def value_function(self):
        return self._value_out

    # ä¾›æˆ‘ä»¬åœ¨ sampler ä¸Šåˆ‡æ¢æ¨ç†æ¨¡å‹ç”¨
    def set_compiled_backbone(self, compiled_bb: nn.Module):
        self.compiled_backbone = compiled_bb
        self.use_compiled = compiled_bb is not None


ModelCatalog.register_custom_model("custom_policy", CustomPolicyNet)


# ============================================================
# ç¼–è¯‘ Hookï¼šåªé’ˆå¯¹ backbone
# ============================================================
class CompressionHook:
    @staticmethod
    def snapshot_backbone(train_model: CustomPolicyNet):
        """
        ä» train_model.backbone æŠ½ snapshotï¼š
        - å¤åˆ¶ state_dictï¼ˆdetach + cpu + cloneï¼‰
        - è®°å½• in_dim / num_outputs
        """
        bb = train_model.backbone
        state_dict_raw = bb.state_dict()
        state_dict = {
            k: v.detach().cpu().clone()
            for k, v in state_dict_raw.items()
        }
        meta = (train_model.in_dim, train_model.num_outputs)
        return state_dict, meta

    @staticmethod
    def build_compiled_backbone(state_dict, meta, backend="inductor"):
        """
        ç»™å®š backbone çš„ snapshotï¼Œæ„é€ ä¸€ä¸ªæ–°çš„ PolicyBackbone å¹¶ç¼–è¯‘ã€‚
        """
        in_dim, num_outputs = meta
        bb = PolicyBackbone(in_dim, num_outputs)
        bb.load_state_dict(state_dict)

        t0 = time.time()
        compiled_bb = torch.compile(bb, backend=backend)
        compile_latency = time.time() - t0

        return compiled_bb, compile_latency


# ============================================================
# PolicyManagerï¼šæ˜¾å¼ç»´æŠ¤è®­ç»ƒæ¨¡å‹ / æ¨ç†éª¨å¹²
# - train_modelï¼šlocal worker ä¸Šçš„ CustomPolicyNetï¼ˆæœªç¼–è¯‘ï¼‰
# - compiled_backboneï¼šä»…ç”¨äº sampler æ¨ç†
# ============================================================
class PolicyManager:
    def __init__(self, algo, mode=CompileMode.NONE, trigger_every=5, backend="inductor"):
        self.algo = algo
        self.mode = mode
        self.trigger_every = trigger_every
        self.backend = backend

        self.lock = threading.Lock()

        # local worker ä¸Šçš„è®­ç»ƒæ¨¡å‹ï¼ˆåªç”¨å®ƒçš„ backboneï¼‰
        self.train_model: CustomPolicyNet = self.algo.get_policy().model

        # å½“å‰ç”Ÿæ•ˆçš„ compiled backboneï¼ˆç”¨äº samplerï¼‰
        self.current_compiled_backbone = None

        # å¼‚æ­¥ pending compiled backbone
        self.pending_compiled_backbone = None
        self.pending_compile_latency = None
        self.pending_copy_latency = None

        # æœ€è¿‘ä¸€æ¬¡çœŸæ­£ç”Ÿæ•ˆçš„ç¼–è¯‘ç»Ÿè®¡ï¼ˆç”¨äº logï¼‰
        self.last_compile_latency = None
        self.last_copy_latency = None
        self.last_swap_latency = None

    # ---------------------- ç¼–è¯‘è§¦å‘é€»è¾‘ ----------------------
    def maybe_trigger_compile(self, epoch: int) -> bool:
        if self.mode == CompileMode.NONE:
            return False
        if epoch % self.trigger_every != 0:
            return False

        if self.mode == CompileMode.SYNC:
            # åŒæ­¥ï¼šç›´æ¥ snapshot + compile + ä¸‹å‘åˆ° sampler
            t_copy0 = time.time()
            state_dict, meta = CompressionHook.snapshot_backbone(self.train_model)
            copy_latency = time.time() - t_copy0

            compiled_bb, compile_latency = CompressionHook.build_compiled_backbone(
                state_dict, meta, backend=self.backend
            )

            self.current_compiled_backbone = compiled_bb
            self.last_compile_latency = compile_latency
            self.last_copy_latency = copy_latency
            self.last_swap_latency = None

            print(
                f"[SYNC Compile] âœ… Snapshot copy={copy_latency:.3f}s, "
                f"compile={compile_latency:.3f}s"
            )

            self._broadcast_compiled_backbone_to_samplers(compiled_bb)

        elif self.mode == CompileMode.ASYNC:
            # å¼‚æ­¥ï¼šåå°çº¿ç¨‹ snapshot + compile
            def worker():
                try:
                    print("[AsyncCompile] ğŸ”§ Start background compilation...")

                    t_copy0 = time.time()
                    with self.lock:
                        state_dict, meta = CompressionHook.snapshot_backbone(self.train_model)
                    copy_latency = time.time() - t_copy0

                    compiled_bb, compile_latency = CompressionHook.build_compiled_backbone(
                        state_dict, meta, backend=self.backend
                    )

                    with self.lock:
                        self.pending_compiled_backbone = compiled_bb
                        self.pending_compile_latency = compile_latency
                        self.pending_copy_latency = copy_latency

                    print(
                        f"[AsyncCompile] âœ… Snapshot copy={copy_latency:.3f}s, "
                        f"compile={compile_latency:.3f}s (waiting to swap)"
                    )

                    del state_dict
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

                except Exception as e:
                    print(f"[AsyncCompile] âŒ Failed: {e}")

            threading.Thread(target=worker, daemon=True).start()

        return True

    # ---------------------- å¼‚æ­¥ swapï¼šåªæ›¿æ¢ sampler ä½¿ç”¨çš„ compiled backbone ----------------------
    def maybe_swap_infer_model(self) -> bool:
        if self.mode != CompileMode.ASYNC:
            return False

        with self.lock:
            if self.pending_compiled_backbone is None:
                return False

            compiled_bb = self.pending_compiled_backbone
            compile_latency = self.pending_compile_latency
            copy_latency = self.pending_copy_latency

            self.pending_compiled_backbone = None
            self.pending_compile_latency = None
            self.pending_copy_latency = None

        # åœ¨é”å¤–è¿›è¡Œå¹¿æ’­ï¼Œé¿å…é•¿æœŸæŒé”
        self.current_compiled_backbone = compiled_bb
        t0 = time.time()
        self._broadcast_compiled_backbone_to_samplers(compiled_bb)
        swap_latency = time.time() - t0

        self.last_compile_latency = compile_latency
        self.last_copy_latency = copy_latency
        self.last_swap_latency = swap_latency

        print(
            f"[AsyncCompile] ğŸ” Swapped sampler compiled_backbone. "
            f"swap_latency={swap_latency:.3f}s"
        )
        return True

    # ---------------------- æŠŠ compiled_backbone æ¨é€åˆ°æ‰€æœ‰ sampler ä¸Š ----------------------
    def _broadcast_compiled_backbone_to_samplers(self, compiled_bb: nn.Module):
        workers = self.algo.workers.remote_workers()

        def _set_compiled(worker):
            # åœ¨è¿œç¨‹ worker ç¯å¢ƒä¸­æ‰§è¡Œ
            def _update_policy(policy, pid):
                if hasattr(policy.model, "set_compiled_backbone"):
                    policy.model.set_compiled_backbone(compiled_bb)
                return 1

            worker.foreach_policy(_update_policy)
            return 1

        ray.get([w.apply.remote(_set_compiled) for w in workers])
        print("[Broadcast] ğŸ“¤ compiled_backbone updated on all sampler workers.")


# ============================================================
# RLTrainerï¼šrollout + train + logging
# ============================================================
class RLTrainer:
    def __init__(self, config, compile_mode=CompileMode.NONE, log_dir="logs", trigger_every=5):
        self.algo = config.build()
        self.compile_mode = compile_mode
        self.manager = PolicyManager(self.algo, compile_mode, trigger_every)

        os.makedirs(log_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_path = os.path.join(log_dir, f"log_{compile_mode.value}_{timestamp}.jsonl")
        self.stats = []
        self.logf = open(self.log_path, "w")

    def _log(self, record):
        json.dump(record, self.logf)
        self.logf.write("\n")
        self.logf.flush()

    def train_epoch(self, epoch: int):
        # 1) ASYNCï¼šçœ‹æœ‰æ²¡æœ‰æ–°çš„ compiled_backbone å¯ä»¥ä¸‹å‘
        swapped = False
        if self.compile_mode == CompileMode.ASYNC:
            swapped = self.manager.maybe_swap_infer_model()

        t0 = time.time()

        # 2) Rolloutï¼šremote workers ç”¨å½“å‰ compiled_backboneï¼ˆè‹¥æœ‰ï¼‰é‡‡æ ·
        workers = self.algo.workers.remote_workers()
        samples = ray.get([w.sample.remote() for w in workers])
        train_batch = concat_samples(samples)
        sample_count = train_batch.count

        # 3) ç¼–è¯‘è§¦å‘
        triggered = self.manager.maybe_trigger_compile(epoch)

        # 4) Trainï¼šlocal worker ç”¨ train_modelï¼ˆæœªç¼–è¯‘ï¼‰è®­ç»ƒ
        result = self.algo.workers.local_worker().learn_on_batch(train_batch)

        t1 = time.time()
        step_time = t1 - t0
        throughput = sample_count / step_time

        # 5) æ—¥å¿—ï¼šåªåœ¨çœŸæ­£ compile / swap çš„ epoch è®°å½• latency
        compile_latency = None
        copy_latency = None
        swap_latency = None

        if self.compile_mode == CompileMode.SYNC and triggered:
            compile_latency = self.manager.last_compile_latency
            copy_latency = self.manager.last_copy_latency
        elif self.compile_mode == CompileMode.ASYNC and swapped:
            compile_latency = self.manager.last_compile_latency
            copy_latency = self.manager.last_copy_latency
            swap_latency = self.manager.last_swap_latency

        log_rec = {
            "epoch": epoch,
            "mode": self.compile_mode.value,
            "reward_mean": result.get("episode_reward_mean", 0.0),
            "train_time": step_time,
            "throughput": throughput,
            "compile_latency": compile_latency,
            "copy_latency": copy_latency,
            "swap_latency": swap_latency,
        }
        self._log(log_rec)
        self.stats.append(log_rec)

        print(
            f"[{self.compile_mode.value.upper()}] Epoch {epoch:<3d} | "
            f"Reward={log_rec['reward_mean']:<8.2f} | "
            f"Samples={sample_count:<6d} | "
            f"Time={step_time:<6.2f}s | "
            f"Throughput={throughput:<8.2f} samples/s | "
            f"Compile={compile_latency}"
        )

    def run(self, num_epochs=10):
        for e in range(1, num_epochs + 1):
            self.train_epoch(e)

    def summary(self):
        print(f"\n=== Summary ({self.compile_mode.value}) ===")
        for s in self.stats:
            print(
                f"Epoch {s['epoch']}: reward={s['reward_mean']:.2f}, "
                f"time={s['train_time']:.2f}s, "
                f"thrpt={s['throughput']:.1f}/s, "
                f"compile={s['compile_latency']}"
            )
        self.logf.close()


# ============================================================
# ä¸»æµç¨‹ï¼šå¯¹æ¯”ä¸‰ç§æ¨¡å¼
# ============================================================
if __name__ == "__main__":
    ray.init(include_dashboard=False, _metrics_export_port=None)

    base_config = (
        PPOConfig()
        .environment(env="CartPole-v1")
        .framework("torch")
        .training(
            model={"custom_model": "custom_policy"},
            train_batch_size=4000,
            lr=3e-4,
        )
    )
    
    # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ Ray API
    try:
        base_config = base_config.env_runners(num_env_runners=2)
    except AttributeError:
        base_config = base_config.rollouts(num_rollout_workers=2)

    trigger_every = 3

    modes = [CompileMode.NONE, CompileMode.SYNC, CompileMode.ASYNC]
    for m in modes:
        print(f"\n=========== Mode = {m.value} ===========")
        trainer = RLTrainer(base_config, compile_mode=m, log_dir="logs", trigger_every=trigger_every)
        trainer.run(num_epochs=5)
        trainer.summary()

    ray.shutdown()