[ASYNC] Epoch   1 | Reward=12.09 | Samples=2000 | Total=24.09s (Rollout=17.60s, Train=6.40s) | Thrpt=83.0/s | Compile=None | Infer=16.726s | Env=0.874s
[ASYNC] Epoch   2 | Reward=9.47 | Samples=2000 | Total=19.54s (Rollout=17.06s, Train=2.40s) | Thrpt=102.4/s | Compile=None | Infer=16.076s | Env=0.988s
Traceback (most recent call last):
  File "/home/mhm/workspace/rllib-accelerator/main.py", line 168, in <module>
    trainer.run(num_epochs=hparams["num_epochs"])
  File "/home/mhm/workspace/rllib-accelerator/framework/trainer.py", line 221, in run
    self.train_epoch(e)
  File "/home/mhm/workspace/rllib-accelerator/framework/trainer.py", line 125, in train_epoch
    meta_swap = self.manager.maybe_swap()
  File "/home/mhm/workspace/rllib-accelerator/framework/policy_manager.py", line 163, in maybe_swap
    self._broadcast_inference_model(infer_model, warmup=warmup and not update_only, update_only=update_only)
  File "/home/mhm/workspace/rllib-accelerator/framework/policy_manager.py", line 137, in _broadcast_inference_model
    ray.get([w.apply.remote(_set) for w in workers])
  File "/home/mhm/.pyenv/versions/3.10.14/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/mhm/.pyenv/versions/3.10.14/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/mhm/.pyenv/versions/3.10.14/lib/python3.10/site-packages/ray/_private/worker.py", line 2626, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name:
	actor_id: 7d624c05ea30f42f2d09ef9c01000000

Traceback (most recent call last):
  File "/home/mhm/workspace/rllib-accelerator/main.py", line 168, in <module>
    trainer.run(num_epochs=hparams["num_epochs"])
  File "/home/mhm/workspace/rllib-accelerator/framework/trainer.py", line 221, in run
    self.train_epoch(e)
  File "/home/mhm/workspace/rllib-accelerator/framework/trainer.py", line 125, in train_epoch
    meta_swap = self.manager.maybe_swap()
  File "/home/mhm/workspace/rllib-accelerator/framework/policy_manager.py", line 163, in maybe_swap
    self._broadcast_inference_model(infer_model, warmup=warmup and not update_only, update_only=update_only)
  File "/home/mhm/workspace/rllib-accelerator/framework/policy_manager.py", line 137, in _broadcast_inference_model
    ray.get([w.apply.remote(_set) for w in workers])
  File "/home/mhm/.pyenv/versions/3.10.14/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/mhm/.pyenv/versions/3.10.14/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/mhm/.pyenv/versions/3.10.14/lib/python3.10/site-packages/ray/_private/worker.py", line 2626, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name:
	actor_id: 7d624c05ea30f42f2d09ef9c01000000

[36m(RolloutWorker pid=720718)[0m W1204 17:36:43.980000 720718 site-packages/torch/utils/cpp_extension.py:118] [0/0] No CUDA runtime is found, using CUDA_HOME='/usr'
[36m(RolloutWorker pid=720720)[0m 2025-12-04 17:35:50,646	WARNING catalog.py:630 -- Custom ModelV2 should accept all custom options as **kwargs, instead of expecting them in config['custom_model_config']![32m [repeated 3x across cluster][0m
