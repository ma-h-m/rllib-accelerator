"""
ç»˜åˆ¶å‰ªæå®éªŒç»“æœ

ç”¨æ³•ï¼š
    python scripts/plot_pruning_results.py --log-dir logs/pruning_basic
"""

import os
import sys
import argparse
import json
from pathlib import Path
from typing import Dict, List

import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm


def load_jsonl(file_path: str) -> List[Dict]:
    """åŠ è½½ JSONL æ–‡ä»¶"""
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


def aggregate_multiple_runs(log_dirs: Dict[str, str]) -> Dict[str, Dict]:
    """
    èšåˆå¤šæ¬¡è¿è¡Œçš„ç»“æœï¼ˆå¤„ç†å¸¦_run0, _run1ç­‰åç¼€çš„æ—¥å¿—ï¼‰
    
    Returns:
        Dict[method_name, Dict] where Dict contains:
            - 'mean': å¹³å‡å€¼æ•°æ®
            - 'std': æ ‡å‡†å·®æ•°æ®
            - 'runs': æ‰€æœ‰è¿è¡Œçš„åŸå§‹æ•°æ®åˆ—è¡¨
    """
    # è¯†åˆ«å®éªŒåç§°ï¼ˆå»é™¤_runåç¼€ï¼‰
    experiment_groups = {}
    
    for label, log_dir in log_dirs.items():
        # æå–åŸºç¡€å®éªŒåï¼ˆå»é™¤_run0, _run1ç­‰ï¼‰
        if '_run' in label:
            base_name = label.rsplit('_run', 1)[0]
        else:
            base_name = label
        
        if base_name not in experiment_groups:
            experiment_groups[base_name] = []
        
        experiment_groups[base_name].append((label, log_dir))
    
    # èšåˆæ¯ç»„å®éªŒ
    aggregated = {}
    
    for base_name, runs in experiment_groups.items():
        if len(runs) == 1:
            # å•æ¬¡è¿è¡Œï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
            label, log_dir = runs[0]
            log_files = list(Path(log_dir).glob("*.jsonl"))
            if log_files:
                data = load_jsonl(log_files[0])
                aggregated[base_name] = {
                    'mean': data,
                    'std': None,
                    'runs': [data],
                    'num_runs': 1
                }
        else:
            # å¤šæ¬¡è¿è¡Œï¼Œè®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
            all_runs_data = []
            
            for label, log_dir in runs:
                log_files = list(Path(log_dir).glob("*.jsonl"))
                if log_files:
                    data = load_jsonl(log_files[0])
                    all_runs_data.append(data)
            
            if not all_runs_data:
                continue
            
            # ç¡®ä¿æ‰€æœ‰è¿è¡Œæœ‰ç›¸åŒçš„epochæ•°
            min_epochs = min(len(run) for run in all_runs_data)
            
            # è®¡ç®—æ¯ä¸ªepochçš„å¹³å‡å€¼å’Œæ ‡å‡†å·®
            mean_data = []
            std_data = []
            
            for epoch_idx in range(min_epochs):
                # æ”¶é›†æ‰€æœ‰è¿è¡Œåœ¨è¿™ä¸ªepochçš„æ•°æ®
                epoch_rewards = [run[epoch_idx]['reward_mean'] for run in all_runs_data]
                epoch_throughputs = [run[epoch_idx].get('throughput', 0) for run in all_runs_data]
                epoch_infer_times = [run[epoch_idx].get('inference_time', 0) for run in all_runs_data]
                epoch_sparsities = [run[epoch_idx].get('sparsity') for run in all_runs_data]
                
                mean_entry = {
                    'epoch': epoch_idx + 1,
                    'reward_mean': np.mean(epoch_rewards),
                    'throughput': np.mean(epoch_throughputs),
                    'inference_time': np.mean(epoch_infer_times),
                }
                
                std_entry = {
                    'epoch': epoch_idx + 1,
                    'reward_std': np.std(epoch_rewards),
                    'throughput_std': np.std(epoch_throughputs),
                    'inference_time_std': np.std(epoch_infer_times),
                }
                
                # å¤„ç†sparsityï¼ˆå¯èƒ½ä¸ºNoneï¼‰
                valid_sparsities = [s for s in epoch_sparsities if s is not None]
                if valid_sparsities:
                    mean_entry['sparsity'] = np.mean(valid_sparsities)
                    std_entry['sparsity_std'] = np.std(valid_sparsities)
                else:
                    mean_entry['sparsity'] = None
                    std_entry['sparsity_std'] = None
                
                mean_data.append(mean_entry)
                std_data.append(std_entry)
            
            aggregated[base_name] = {
                'mean': mean_data,
                'std': std_data,
                'runs': all_runs_data,
                'num_runs': len(all_runs_data)
            }
    
    return aggregated


def smooth_curve(values, weight=0.9):
    """æŒ‡æ•°ç§»åŠ¨å¹³å‡å¹³æ»‘"""
    smoothed = []
    last = values[0]
    for v in values:
        smoothed_val = last * weight + (1 - weight) * v
        smoothed.append(smoothed_val)
        last = smoothed_val
    return smoothed


def plot_reward_comparison(log_dirs: Dict[str, str], output_path: str = None):
    """ç»˜åˆ¶ reward å¯¹æ¯”å›¾"""
    fig, ax = plt.subplots(figsize=(12, 6))
    
    colors = cm.get_cmap('tab10')
    
    for idx, (label, log_dir) in enumerate(log_dirs.items()):
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ª jsonl æ–‡ä»¶
        log_files = list(Path(log_dir).glob("*.jsonl"))
        if not log_files:
            print(f"âš ï¸ No log files found in {log_dir}")
            continue
        
        log_file = log_files[0]
        data = load_jsonl(log_file)
        
        epochs = [d['epoch'] for d in data]
        rewards = [d.get('reward_mean', 0) for d in data]
        
        # ç»˜åˆ¶åŸå§‹æ›²çº¿ï¼ˆåŠé€æ˜ï¼‰
        ax.plot(epochs, rewards, alpha=0.2, color=colors(idx))
        
        # ç»˜åˆ¶å¹³æ»‘æ›²çº¿
        if len(rewards) > 1:
            smoothed = smooth_curve(rewards, weight=0.9)
            ax.plot(epochs, smoothed, label=label, linewidth=2, color=colors(idx))
        else:
            ax.plot(epochs, rewards, label=label, linewidth=2, color=colors(idx))
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Average Reward', fontsize=12)
    ax.set_title('Training Reward Comparison', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Saved reward plot to {output_path}")
    else:
        plt.show()
    
    plt.close()


def plot_throughput_comparison(log_dirs: Dict[str, str], output_path: str = None):
    """ç»˜åˆ¶ååé‡å¯¹æ¯”å›¾"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    labels = []
    throughputs = []
    
    for label, log_dir in log_dirs.items():
        log_files = list(Path(log_dir).glob("*.jsonl"))
        if not log_files:
            continue
        
        log_file = log_files[0]
        data = load_jsonl(log_file)
        
        # è®¡ç®—å¹³å‡ååé‡ï¼ˆè·³è¿‡å‰å‡ ä¸ª epoch çš„é¢„çƒ­ï¼‰
        skip_epochs = min(5, len(data) // 10)
        throughput_values = [d.get('throughput', 0) for d in data[skip_epochs:]]
        avg_throughput = np.mean(throughput_values)
        
        labels.append(label)
        throughputs.append(avg_throughput)
    
    # ç»˜åˆ¶æ¡å½¢å›¾
    x = np.arange(len(labels))
    bars = ax.bar(x, throughputs, color='steelblue', alpha=0.7)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, val in zip(bars, throughputs):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{val:.1f}',
                ha='center', va='bottom', fontsize=10)
    
    ax.set_xlabel('Method', fontsize=12)
    ax.set_ylabel('Throughput (samples/s)', fontsize=12)
    ax.set_title('Average Throughput Comparison', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Saved throughput plot to {output_path}")
    else:
        plt.show()
    
    plt.close()


def plot_inference_time_comparison(log_dirs: Dict[str, str], output_path: str = None):
    """ç»˜åˆ¶æ¨ç†æ—¶é—´å¯¹æ¯”å›¾"""
    fig, ax = plt.subplots(figsize=(10, 6))
    
    labels = []
    inference_times = []
    
    for label, log_dir in log_dirs.items():
        log_files = list(Path(log_dir).glob("*.jsonl"))
        if not log_files:
            continue
        
        log_file = log_files[0]
        data = load_jsonl(log_file)
        
        # è®¡ç®—å¹³å‡æ¨ç†æ—¶é—´
        skip_epochs = min(5, len(data) // 10)
        infer_values = [d.get('inference_time', 0) for d in data[skip_epochs:]]
        avg_infer = np.mean(infer_values)
        
        labels.append(label)
        inference_times.append(avg_infer * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
    
    # ç»˜åˆ¶æ¡å½¢å›¾
    x = np.arange(len(labels))
    bars = ax.bar(x, inference_times, color='coral', alpha=0.7)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, val in zip(bars, inference_times):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{val:.1f}ms',
                ha='center', va='bottom', fontsize=10)
    
    ax.set_xlabel('Method', fontsize=12)
    ax.set_ylabel('Inference Time (ms)', fontsize=12)
    ax.set_title('Average Inference Time Comparison', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(labels, rotation=45, ha='right')
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Saved inference time plot to {output_path}")
    else:
        plt.show()
    
    plt.close()


def plot_compression_ratio(log_dirs: Dict[str, str], output_path: str = None):
    """ç»˜åˆ¶å‹ç¼©ç‡å˜åŒ–å›¾ï¼ˆä»…é€‚ç”¨äºå‰ªæå®éªŒï¼‰"""
    fig, ax = plt.subplots(figsize=(12, 6))
    
    colors = cm.get_cmap('tab10')
    
    for idx, (label, log_dir) in enumerate(log_dirs.items()):
        if 'prune' not in label.lower():
            continue  # åªå¤„ç†å‰ªæç›¸å…³çš„å®éªŒ
        
        log_files = list(Path(log_dir).glob("*.jsonl"))
        if not log_files:
            continue
        
        log_file = log_files[0]
        data = load_jsonl(log_file)
        
        # æå–å‰ªæä¿¡æ¯
        epochs = []
        ratios = []
        
        for d in data:
            # å°è¯•ä»æ—¥å¿—ä¸­æå–å‹ç¼©ç‡ä¿¡æ¯
            # æ³¨æ„ï¼šè¿™éœ€è¦åœ¨å‹ç¼©æ—¶è®°å½•åˆ°æ—¥å¿—ä¸­
            epoch = d.get('epoch')
            # å¦‚æœæœ‰ compression_ratio å­—æ®µ
            ratio = d.get('compression_ratio')
            if ratio is not None:
                epochs.append(epoch)
                ratios.append(ratio)
        
        if epochs:
            ax.plot(epochs, ratios, label=label, linewidth=2, 
                   color=colors(idx), marker='o', markersize=4)
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Compression Ratio (remaining neurons)', fontsize=12)
    ax.set_title('Model Compression Over Time', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0, 1.1])
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… Saved compression ratio plot to {output_path}")
    else:
        plt.show()
    
    plt.close()


def plot_pruning_ratio_comparison(log_dirs: Dict[str, str], output_path: str = None):
    """ç»˜åˆ¶ä¸åŒpruning ratioçš„å¯¹æ¯”å›¾ï¼ˆæŸ±çŠ¶å›¾ï¼‰"""
    # æå–ratioå®éªŒçš„æ•°æ®
    ratio_data = {}
    baseline_data = None
    
    for label, log_dir in log_dirs.items():
        log_files = list(Path(log_dir).glob("*.jsonl"))
        if not log_files:
            continue
        
        log_file = log_files[0]
        data = load_jsonl(log_file)
        skip = min(5, len(data) // 10)
        
        # è®¡ç®—æŒ‡æ ‡
        rewards = [d.get('reward_mean', 0) for d in data[-50:]]
        throughputs = [d.get('throughput', 0) for d in data[skip:]]
        sparsities = [d.get('sparsity', 0) for d in data if d.get('sparsity') is not None]
        
        avg_reward = np.mean(rewards)
        avg_throughput = np.mean(throughputs)
        avg_sparsity = np.mean(sparsities) if sparsities else 0
        
        if 'baseline' in label.lower():
            baseline_data = {
                'reward': avg_reward,
                'throughput': avg_throughput,
                'sparsity': 0
            }
        elif 'ratio=' in label:
            # æå–ratioå€¼
            try:
                ratio_str = label.split('ratio=')[1].split('_')[0]
                ratio = float(ratio_str)
                ratio_data[ratio] = {
                    'reward': avg_reward,
                    'throughput': avg_throughput,
                    'sparsity': avg_sparsity
                }
            except:
                pass
    
    if not ratio_data:
        print("âš ï¸ No pruning ratio experiments found")
        return
    
    # æ’åº
    ratios = sorted(ratio_data.keys())
    rewards = [ratio_data[r]['reward'] for r in ratios]
    throughputs = [ratio_data[r]['throughput'] for r in ratios]
    sparsities = [ratio_data[r]['sparsity'] * 100 for r in ratios]  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Pruning Ratio Comparison', fontsize=16, fontweight='bold')
    
    # 1. Avg Reward vs Pruning Ratio
    ax = axes[0, 0]
    bars = ax.bar([f'{r:.1f}' for r in ratios], rewards, color='steelblue', alpha=0.7, edgecolor='black')
    if baseline_data:
        ax.axhline(y=baseline_data['reward'], color='red', linestyle='--', 
                   linewidth=2, label=f"Baseline ({baseline_data['reward']:.1f})")
        ax.legend()
    ax.set_xlabel('Pruning Ratio', fontsize=12)
    ax.set_ylabel('Average Reward (last 50 epochs)', fontsize=12)
    ax.set_title('Performance vs Pruning Ratio', fontsize=13, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, reward in zip(bars, rewards):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{reward:.1f}', ha='center', va='bottom', fontsize=10)
    
    # 2. Throughput vs Pruning Ratio
    ax = axes[0, 1]
    bars = ax.bar([f'{r:.1f}' for r in ratios], throughputs, color='coral', alpha=0.7, edgecolor='black')
    if baseline_data:
        ax.axhline(y=baseline_data['throughput'], color='red', linestyle='--', 
                   linewidth=2, label=f"Baseline ({baseline_data['throughput']:.0f})")
        ax.legend()
    ax.set_xlabel('Pruning Ratio', fontsize=12)
    ax.set_ylabel('Throughput (samples/s)', fontsize=12)
    ax.set_title('Throughput vs Pruning Ratio', fontsize=13, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    # 3. Sparsity vs Pruning Ratio
    ax = axes[1, 0]
    bars = ax.bar([f'{r:.1f}' for r in ratios], sparsities, color='mediumseagreen', alpha=0.7, edgecolor='black')
    ax.set_xlabel('Pruning Ratio', fontsize=12)
    ax.set_ylabel('Actual Sparsity (%)', fontsize=12)
    ax.set_title('Achieved Sparsity', fontsize=13, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    
    for bar, sparsity in zip(bars, sparsities):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{sparsity:.1f}%', ha='center', va='bottom', fontsize=10)
    
    # 4. Reward vs Sparsity Trade-off
    ax = axes[1, 1]
    ax.scatter(sparsities, rewards, s=200, c=ratios, cmap='viridis', 
               alpha=0.7, edgecolors='black', linewidth=2)
    if baseline_data:
        ax.scatter([0], [baseline_data['reward']], s=200, c='red', 
                   marker='*', edgecolors='black', linewidth=2, label='Baseline', zorder=5)
    
    # æ·»åŠ æ ‡ç­¾
    for ratio, sparsity, reward in zip(ratios, sparsities, rewards):
        ax.annotate(f'ratio={ratio:.1f}', 
                   (sparsity, reward), 
                   xytext=(5, 5), textcoords='offset points',
                   fontsize=9, alpha=0.8)
    
    ax.set_xlabel('Sparsity (%)', fontsize=12)
    ax.set_ylabel('Average Reward', fontsize=12)
    ax.set_title('Performance vs Sparsity Trade-off', fontsize=13, fontweight='bold')
    ax.grid(True, alpha=0.3)
    if baseline_data:
        ax.legend()
    
    plt.tight_layout()
    
    if output_path:
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        print(f"âœ… Saved pruning ratio comparison to {output_path}")
    else:
        plt.show()
    
    plt.close()


def print_summary_table(log_dirs: Dict[str, str]):
    """æ‰“å°æ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "="*80)
    print("PERFORMANCE SUMMARY")
    print("="*80)
    print(f"{'Method':<30} {'Avg Reward':<15} {'Throughput':<15} {'Inference Time':<15}")
    print("-"*80)
    
    for label, log_dir in log_dirs.items():
        log_files = list(Path(log_dir).glob("*.jsonl"))
        if not log_files:
            continue
        
        log_file = log_files[0]
        data = load_jsonl(log_file)
        
        skip = min(5, len(data) // 10)
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        rewards = [d.get('reward_mean', 0) for d in data[-50:]]  # æœ€å 50 ä¸ª epoch
        throughputs = [d.get('throughput', 0) for d in data[skip:]]
        infer_times = [d.get('inference_time', 0) for d in data[skip:]]
        
        avg_reward = np.mean(rewards)
        avg_throughput = np.mean(throughputs)
        avg_infer = np.mean(infer_times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        print(f"{label:<30} {avg_reward:<15.2f} {avg_throughput:<15.1f} {avg_infer:<15.2f}ms")
    
    print("="*80 + "\n")


def main():
    parser = argparse.ArgumentParser(description="Plot pruning experiment results")
    parser.add_argument(
        "--log-dir",
        type=str,
        required=True,
        help="Directory containing experiment logs"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="Directory to save plots (default: same as log-dir)"
    )
    
    args = parser.parse_args()
    
    log_dir = Path(args.log_dir)
    if not log_dir.exists():
        print(f"âŒ Log directory not found: {log_dir}")
        return
    
    # è‡ªåŠ¨å‘ç°æ‰€æœ‰å®éªŒå­ç›®å½•
    log_dirs = {}
    for subdir in sorted(log_dir.iterdir()):
        if subdir.is_dir():
            # ä½¿ç”¨ç›®å½•åä½œä¸ºæ ‡ç­¾
            label = subdir.name
            log_dirs[label] = str(subdir)
    
    if not log_dirs:
        print(f"âŒ No experiment subdirectories found in {log_dir}")
        return
    
    print(f"Found {len(log_dirs)} experiments:")
    for label in log_dirs.keys():
        print(f"  - {label}")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir) if args.output_dir else log_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æµ‹å®éªŒç±»å‹
    is_ratio_experiment = any('ratio=' in label for label in log_dirs.keys())
    
    # ç”Ÿæˆæ‰€æœ‰å›¾è¡¨
    print("\nGenerating plots...")
    plot_reward_comparison(log_dirs, str(output_dir / "reward_comparison.png"))
    plot_throughput_comparison(log_dirs, str(output_dir / "throughput_comparison.png"))
    plot_inference_time_comparison(log_dirs, str(output_dir / "inference_time_comparison.png"))
    
    # å¦‚æœæ˜¯pruning ratioå®éªŒï¼Œç”Ÿæˆä¸“é—¨çš„å¯¹æ¯”å›¾
    if is_ratio_experiment:
        print("\nğŸ“Š Detected pruning ratio experiment, generating ratio comparison plots...")
        plot_pruning_ratio_comparison(log_dirs, str(output_dir / "pruning_ratio_comparison.png"))
    
    # æ‰“å°æ€§èƒ½å¯¹æ¯”è¡¨
    print_summary_table(log_dirs)
    
    print(f"\nâœ… All plots saved to: {output_dir}")


if __name__ == "__main__":
    main()

